  0%|                                                                                                                                                                   | 0/2 [00:00<?, ?it/s]/home/ksj3535/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
> /home/ksj3535/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py(642)forward()
-> if attention_mask is not None:
> /home/ksj3535/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py(642)forward()
-> if attention_mask is not None:
> /home/ksj3535/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py(642)forward()
-> if attention_mask is not None:
> /home/ksj3535/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py(642)forward()
-> if attention_mask is not None:
> /home/ksj3535/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py(642)forward()
-> if attention_mask is not None:
tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [0, 0, 0,  ..., 1, 1, 1],
        [0, 0, 0,  ..., 1, 1, 1]], device='cuda:0')
Error executing job with overrides: []
Traceback (most recent call last):
  File "/home/ksj3535/research/RestEM/src/train.py", line 35, in main
    trainer.train()
  File "/home/ksj3535/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/trainer.py", line 2052, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ksj3535/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/trainer.py", line 2388, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ksj3535/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/trainer.py", line 3485, in training_step
    loss = self.compute_loss(model, inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ksj3535/research/RestEM/src/train.py", line 12, in compute_loss
    outputs = model(**input)
              ^^^^^^^^^^^^^^
  File "/home/ksj3535/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ksj3535/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ksj3535/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py", line 186, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ksj3535/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py", line 201, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ksj3535/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py", line 108, in parallel_apply
    output.reraise()
  File "/home/ksj3535/anaconda3/envs/llm/lib/python3.11/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
bdb.BdbQuit: Caught BdbQuit in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ksj3535/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ksj3535/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ksj3535/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ksj3535/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1190, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/home/ksj3535/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ksj3535/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ksj3535/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1001, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/home/ksj3535/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ksj3535/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ksj3535/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 730, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
                                                          ^^^^^^^^^^^^^^^
  File "/home/ksj3535/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ksj3535/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ksj3535/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 642, in forward
    if attention_mask is not None:
       ^^^^^^^^^^^^^^
  File "/home/ksj3535/anaconda3/envs/llm/lib/python3.11/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ksj3535/anaconda3/envs/llm/lib/python3.11/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
                      ^^^^^^^^^^^^^
bdb.BdbQuit


Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
