model_name_or_path: "unsloth/llama-3-8b-instruct"
dataset_path: "outputs/sample_output.jsonl"
attention_impl: "flash_attention_2"
trainer:
  output_dir: "outputs"
  do_train: true
  eval_strategy: "no"
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 4
  learning_rate: 1e-5
  num_train_epochs: 1
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.1
  logging_strategy: "steps"
  save_strategy: "epoch"
  fp16: false
  bf16: true
  run_name: "llama-3-8b-instruct"
  group_by_length: true
  report_to: "none"
  deepspeed: "configs/deepspeed_config.json"
